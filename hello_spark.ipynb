{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://spark.apache.org/docs/latest/quick-start.html\n",
    "textFile = spark.read.text(\"README.md\") # Creates a DataFrame from the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n"
     ]
    }
   ],
   "source": [
    "print (textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()  # Number of rows in this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# PySpark In Jupyter')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()  # First row in this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n"
     ]
    }
   ],
   "source": [
    "# transform this DataFrame to a new one\n",
    "linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\")) \n",
    "print (linesWithSpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# action count() is called on the new DataFrame\n",
    "print (linesWithSpark.count()) # How many lines contain \"Spark\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=40)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This first maps a line to an integer value and aliases it as “numWords”, creating a new DataFrame. \n",
    "# agg is called on that DataFrame to find the largest word count. \n",
    "# The arguments to select and agg are both Column, we can use df.colName to get a column from a DataFrame. \n",
    "# We can also import pyspark.sql.functions, which provides a lot of convenient functions to build a new Column from an old one.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='`$', count=3),\n",
       " Row(word='version,', count=1),\n",
       " Row(word='/usr/local/Cellar/spark/x.y.z', count=1),\n",
       " Row(word='port', count=2),\n",
       " Row(word='[Jupyter', count=1),\n",
       " Row(word='local', count=2),\n",
       " Row(word='Copy', count=1),\n",
       " Row(word='server', count=1),\n",
       " Row(word='down', count=1),\n",
       " Row(word='Jupyter](https://towardsdatascience.com/how-to-use-pyspark-on-your-computer-9c7180075617)', count=1),\n",
       " Row(word='done,', count=1),\n",
       " Row(word='8888', count=1),\n",
       " Row(word=\"PYSPARK_DRIVER_PYTHON_OPTS=notebook'\", count=1),\n",
       " Row(word='1.8.0.242', count=1),\n",
       " Row(word='will', count=1),\n",
       " Row(word='particular', count=1),\n",
       " Row(word='container', count=1),\n",
       " Row(word='process', count=1),\n",
       " Row(word='Jypyter', count=1),\n",
       " Row(word='using', count=1),\n",
       " Row(word='1.8.0.242,', count=1),\n",
       " Row(word=\"'export\", count=6),\n",
       " Row(word='variable', count=1),\n",
       " Row(word='ln', count=1),\n",
       " Row(word='In', count=1),\n",
       " Row(word='[jEnv](https://github.com/jenv/jenv)', count=1),\n",
       " Row(word='>>', count=7),\n",
       " Row(word='Docker', count=1),\n",
       " Row(word='3.', count=1),\n",
       " Row(word='http://spark.apache.org/downloads.html', count=1),\n",
       " Row(word='add', count=4),\n",
       " Row(word='spark-x.y.z-bin-hadoopx.y', count=1),\n",
       " Row(word='can', count=2),\n",
       " Row(word='was', count=1),\n",
       " Row(word='configure', count=1),\n",
       " Row(word='for', count=2),\n",
       " Row(word='[PySpark', count=1),\n",
       " Row(word='system', count=1),\n",
       " Row(word='--rm', count=1),\n",
       " Row(word='-it', count=1),\n",
       " Row(word='exited', count=1),\n",
       " Row(word='in', count=1),\n",
       " Row(word='/usr/local/Cellar/spark/x.y.z`', count=1),\n",
       " Row(word='with', count=2),\n",
       " Row(word='machine', count=1),\n",
       " Row(word='say', count=1),\n",
       " Row(word='had', count=1),\n",
       " Row(word='####', count=5),\n",
       " Row(word='(Mac).', count=1),\n",
       " Row(word='be', count=3),\n",
       " Row(word='So', count=1),\n",
       " Row(word='jupyter/pyspark-notebook', count=1),\n",
       " Row(word='jupyter`', count=1),\n",
       " Row(word='Note:', count=2),\n",
       " Row(word=\"'eval\", count=1),\n",
       " Row(word='/path/to/project/dir/', count=1),\n",
       " Row(word='-)\"\\'', count=1),\n",
       " Row(word='sending', count=1),\n",
       " Row(word='link', count=1),\n",
       " Row(word='docker', count=1),\n",
       " Row(word='number.', count=1),\n",
       " Row(word='PATH', count=1),\n",
       " Row(word='cd', count=1),\n",
       " Row(word='Download', count=1),\n",
       " Row(word='source', count=3),\n",
       " Row(word='/usr/local/opt/spark`', count=1),\n",
       " Row(word='versions', count=1),\n",
       " Row(word='project', count=2),\n",
       " Row(word='variables', count=1),\n",
       " Row(word='Stacks](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html)', count=1),\n",
       " Row(word=\"I've\", count=1),\n",
       " Row(word='Apache', count=1),\n",
       " Row(word='on', count=3),\n",
       " Row(word='pip3', count=1),\n",
       " Row(word='managed', count=1),\n",
       " Row(word='tar', count=1),\n",
       " Row(word='(On', count=1),\n",
       " Row(word='-s', count=1),\n",
       " Row(word='Set', count=1),\n",
       " Row(word='/Library/Java/JavaVirtualMachines/openjdk64-1.8.0.241.jdk/Contents/Home', count=1),\n",
       " Row(word='jenv', count=7),\n",
       " Row(word='error:', count=1),\n",
       " Row(word='Setup', count=1),\n",
       " Row(word='far', count=1),\n",
       " Row(word='steps', count=1),\n",
       " Row(word='following', count=1),\n",
       " Row(word='/Library/Java/JavaVirtualMachines/openjdk64-13.0.2.jdk/Contents/Home', count=1),\n",
       " Row(word='Add', count=2),\n",
       " Row(word=\"PYSPARK_DRIVER_PYTHON=jupyter'\", count=1),\n",
       " Row(word='listening', count=1),\n",
       " Row(word='its', count=1),\n",
       " Row(word='-', count=4),\n",
       " Row(word='$PATH', count=1),\n",
       " Row(word='You', count=1),\n",
       " Row(word='gateway', count=1),\n",
       " Row(word='5.', count=1),\n",
       " Row(word='brew', count=1),\n",
       " Row(word='quickly', count=1),\n",
       " Row(word='pyspark', count=1),\n",
       " Row(word='the', count=5),\n",
       " Row(word='To', count=1),\n",
       " Row(word='List', count=1),\n",
       " Row(word='-p', count=1),\n",
       " Row(word='Exception:', count=1),\n",
       " Row(word='my', count=1),\n",
       " Row(word='redirected', count=1),\n",
       " Row(word=\"PYSPARK_PYTHON=python3'\", count=1),\n",
       " Row(word='way..', count=1),\n",
       " Row(word='`mv', count=1),\n",
       " Row(word='init', count=1),\n",
       " Row(word='what', count=1),\n",
       " Row(word='work', count=1),\n",
       " Row(word='JDKs', count=4),\n",
       " Row(word='1.8+.', count=1),\n",
       " Row(word='\"$(jenv', count=1),\n",
       " Row(word='http://localhost:8888/tree', count=1),\n",
       " Row(word='fail', count=1),\n",
       " Row(word='getting', count=1),\n",
       " Row(word='and', count=3),\n",
       " Row(word='Update', count=1),\n",
       " Row(word='driver', count=1),\n",
       " Row(word='setup', count=1),\n",
       " Row(word='location', count=1),\n",
       " Row(word='Launch', count=1),\n",
       " Row(word='through', count=1),\n",
       " Row(word='Start', count=1),\n",
       " Row(word='Notebook', count=1),\n",
       " Row(word='PySpark', count=3),\n",
       " Row(word='```', count=20),\n",
       " Row(word='Unzip', count=1),\n",
       " Row(word='/Library/Java/JavaVirtualMachines/openjdk64-1.8.0.242.jdk/Contents/Home', count=1),\n",
       " Row(word='#', count=1),\n",
       " Row(word='###', count=2),\n",
       " Row(word='install', count=2),\n",
       " Row(word='symbolic', count=1),\n",
       " Row(word='before', count=1),\n",
       " Row(word='~/.bash_profile', count=10),\n",
       " Row(word='that', count=1),\n",
       " Row(word='I', count=3),\n",
       " Row(word='running', count=1),\n",
       " Row(word='spark-x.y.z-bin-hadoopx.y.tgz`', count=1),\n",
       " Row(word='PATH=\"$SPARK_HOME/bin:$PATH\"\\'', count=1),\n",
       " Row(word='8888:8888', count=1),\n",
       " Row(word='run', count=1),\n",
       " Row(word='2.', count=1),\n",
       " Row(word='Jupyter', count=2),\n",
       " Row(word='Spark', count=4),\n",
       " Row(word='achieved', count=1),\n",
       " Row(word='standard', count=1),\n",
       " Row(word='java', count=3),\n",
       " Row(word='all', count=1),\n",
       " Row(word='`SPARK_HOME`', count=1),\n",
       " Row(word='a', count=3),\n",
       " Row(word='echo', count=7),\n",
       " Row(word='--version', count=1),\n",
       " Row(word='', count=28),\n",
       " Row(word='$', count=19),\n",
       " Row(word='6.', count=1),\n",
       " Row(word='1.', count=8),\n",
       " Row(word='-xzf', count=1),\n",
       " Row(word='Java', count=3),\n",
       " Row(word='Verify', count=1),\n",
       " Row(word='environment', count=2),\n",
       " Row(word='Mac)', count=1),\n",
       " Row(word='might', count=1),\n",
       " Row(word='Manage', count=1),\n",
       " Row(word='4.', count=1),\n",
       " Row(word='notebook', count=1),\n",
       " Row(word='Install', count=2),\n",
       " Row(word='to', count=7),\n",
       " Row(word='1.13.0', count=1),\n",
       " Row(word='Create', count=1),\n",
       " Row(word='multiple', count=2),\n",
       " Row(word='version', count=1),\n",
       " Row(word='jotted', count=1),\n",
       " Row(word='Multiple', count=1),\n",
       " Row(word='PATH=\"$HOME/.jenv/bin:$PATH\"\\'', count=1),\n",
       " Row(word=\"SPARK_HOME=/usr/local/opt/spark'\", count=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: MapReduce, Spark can implement MapReduce flows easily\n",
    "# Here, we use the explode function in select, to transform a Dataset of lines to a Dataset of words, \n",
    "# and then combine groupBy and count to compute the per-word counts in the file as a DataFrame of 2 columns: “word” and “count”. \n",
    "wordCounts = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "\n",
    "# To collect the word counts in our shell, we can call collect\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
